{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/negarhonarvar/Equation-Solving-OCR/blob/main/EquationSolving_OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cEzoc6TXFm8"
      },
      "source": [
        "# Equation Detection and Solving with OCR\n",
        "In this task, we shall classify equations into two categories:\n",
        "\n",
        "\n",
        "*   Handwritten\n",
        "*   Typped\n",
        "\n",
        "afterwards, we shall solve each equation and report the results of it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBeanDVEXQ56"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOeI3KdiV8rc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVB2rc0dX6dm"
      },
      "source": [
        "## Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOYiX5iFX5kp",
        "outputId": "8661e80c-42dc-41b1-f19c-2b7fdbeeff3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/OCR_data\"\n",
        "TRAIN_DIR = os.path.join(DRIVE_DIR, \"train\")\n",
        "TEST_DIR = os.path.join(DRIVE_DIR, \"test\")\n",
        "TRAIN_CSV = os.path.join(DRIVE_DIR, \"train_info.csv\")\n",
        "SUBMISSION_CSV = os.path.join(DRIVE_DIR, \"submission.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS0_E0P8X-5d"
      },
      "source": [
        "## HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NUpIKWyYB6h"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 20\n",
        "LR = 1e-4\n",
        "VAL_SPLIT = 0.1\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srLOqST9YFWV"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i529PPCeYixY"
      },
      "source": [
        "Reading the train_info.csv file and extracting information and details on training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh6Yd-VBYMWj"
      },
      "outputs": [],
      "source": [
        "def read_train_csv(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DsZW7vVYrW6"
      },
      "source": [
        "we duplicated the minority class of Handwritten data (only 150) to balance our dataset and prevent overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG6uyEDNY5yg"
      },
      "outputs": [],
      "source": [
        "def balance_dataset(df):\n",
        "    typed_df = df[df[\"type\"] == 0]\n",
        "    handwritten_df = df[df[\"type\"] == 1]\n",
        "    len_typed = len(typed_df)\n",
        "    len_hand = len(handwritten_df)\n",
        "    if len_hand < len_typed:\n",
        "        factor = math.ceil(len_typed / len_hand)\n",
        "        oversampled = pd.concat([handwritten_df]*factor, ignore_index=True)\n",
        "        balanced = pd.concat([typed_df, oversampled], ignore_index=True)\n",
        "    else:\n",
        "        factor = math.ceil(len_hand / len_typed)\n",
        "        oversampled = pd.concat([typed_df]*factor, ignore_index=True)\n",
        "        balanced = pd.concat([oversampled, handwritten_df], ignore_index=True)\n",
        "    balanced = balanced.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "    return balanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ru_B6aOZKGr"
      },
      "source": [
        "The class below implements a torch dataset which helps us for the classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg4GpCk2ZR6m"
      },
      "outputs": [],
      "source": [
        "class ExpressionTypeDataset(Dataset):\n",
        "    def __init__(self, df, root_dir, transform=None):\n",
        "\n",
        "        df = df[df[\"path\"].apply(lambda x: os.path.exists(os.path.join(root_dir, x)))]\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        filename = row[\"path\"]\n",
        "        label = int(row[\"type\"])    # 0=typed,1=handwritten\n",
        "        expression = row[\"answer\"]\n",
        "        path = os.path.join(self.root_dir, filename)\n",
        "        pil_img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img_tensor = self.transform(pil_img)\n",
        "        else:\n",
        "            img_tensor = T.ToTensor()(pil_img)\n",
        "        return img_tensor, label, expression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EixXqdHfZXv9"
      },
      "source": [
        "Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx2t_yvUZXRe"
      },
      "outputs": [],
      "source": [
        "train_transforms = T.Compose([\n",
        "    T.Resize((224,224)),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(10),\n",
        "    T.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485,0.456,0.406],\n",
        "                [0.229,0.224,0.225])\n",
        "])\n",
        "val_transforms = T.Compose([\n",
        "    T.Resize((224,224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485,0.456,0.406],\n",
        "                [0.229,0.224,0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks1aNxYAZdcL"
      },
      "source": [
        "## Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN0q-_8IZfoO"
      },
      "outputs": [],
      "source": [
        "def create_type_model(num_classes=2):\n",
        "    model = torchvision.models.resnet18(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlxetw6FZjOn"
      },
      "source": [
        "## Character Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P__Hw8CpZpyJ"
      },
      "source": [
        "We'll assume we have a separate folder structure\n",
        "\n",
        "\"char_train\" with subfolders \"0\",\"1\",\"2\",...,\"9\",\"plus\",\"minus\",\"times\",\"div\",\"lparen\",\"rparen\" ,\n",
        "to train a single-character classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap6wAcm6ZmEK"
      },
      "outputs": [],
      "source": [
        "class SingleCharDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        subfolders = sorted(os.listdir(root_dir))\n",
        "        label_map = {}\n",
        "\n",
        "        # mapping\n",
        "        #  0->\"0\", 1->\"1\",..., 9->\"9\", 10->\"plus\", 11->\"minus\",12->\"times\",13->\"div\",14->\"lparen\",15->\"rparen\"\n",
        "\n",
        "        idx = 0\n",
        "        for subf in subfolders:\n",
        "            label_map[subf] = idx\n",
        "            idx+=1\n",
        "        for subf in subfolders:\n",
        "            sub_path = os.path.join(root_dir, subf)\n",
        "            if not os.path.isdir(sub_path):\n",
        "                continue\n",
        "            label = label_map[subf]\n",
        "            for file in os.listdir(sub_path):\n",
        "                if file.lower().endswith(('.png','.jpg','.jpeg')):\n",
        "                    self.samples.append((os.path.join(sub_path, file), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        pil_img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img_tensor = self.transform(pil_img)\n",
        "        else:\n",
        "            img_tensor = T.ToTensor()(pil_img)\n",
        "        return img_tensor, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCMQn0CKa-Po"
      },
      "source": [
        "### Character Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6m9O_-uZ9Eu"
      },
      "outputs": [],
      "source": [
        "def create_char_model(num_classes=16):\n",
        "    model = torchvision.models.resnet18(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPiN8wvfbCqL"
      },
      "source": [
        "### Character Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP0RnnpobFkU"
      },
      "outputs": [],
      "source": [
        "def segment_characters(pil_img):\n",
        "    # we convert data to grayscale, threshold, find contours\n",
        "\n",
        "    img_cv = np.array(pil_img.convert(\"L\"))\n",
        "    _, thresh = cv2.threshold(img_cv, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    bboxes = []\n",
        "    for cnt in contours:\n",
        "        x,y,w,h = cv2.boundingRect(cnt)\n",
        "\n",
        "        if w<5 or h<5:\n",
        "            continue\n",
        "        bboxes.append((x,y,w,h))\n",
        "\n",
        "    bboxes.sort(key=lambda b: b[0])\n",
        "    char_images = []\n",
        "    for (x,y,w,h) in bboxes:\n",
        "        crop = img_cv[y:y+h, x:x+w]\n",
        "\n",
        "        pil_crop = Image.fromarray(crop)\n",
        "        char_images.append((x, pil_crop))\n",
        "    return char_images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-csCQIpbRNh"
      },
      "source": [
        "### Character Classification into Bounding Box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugpOnn3zbZGa"
      },
      "outputs": [],
      "source": [
        "def classify_characters(char_images, model, transform, label_map_rev):\n",
        "\n",
        "    # char_images is list of (x, PILimage)\n",
        "    # label_map_rev is like this = {0:'0',1:'1',...10:'plus',11:'minus',12:'times',13:'div',14:'lparen',15:'rparen'}\n",
        "\n",
        "    results = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (xpos, pil_img) in char_images:\n",
        "            rgb_img = pil_img.convert(\"RGB\")\n",
        "            tensor_img = transform(rgb_img).unsqueeze(0).to(DEVICE)\n",
        "            out = model(tensor_img)\n",
        "            _, pred = torch.max(out, 1)\n",
        "            pred_label = pred.item()\n",
        "            results.append((xpos, label_map_rev[pred_label]))\n",
        "\n",
        "    results.sort(key=lambda r: r[0])\n",
        "\n",
        "    recognized = [r[1] for r in results]\n",
        "    return recognized\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PoI0aTjblBf"
      },
      "source": [
        "### To string Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eQH16lxbnds"
      },
      "outputs": [],
      "source": [
        "def symbols_to_expression(symbols):\n",
        "\n",
        "    op_map = {\n",
        "        'plus': '+',\n",
        "        'minus': '-',\n",
        "        'times': '×',\n",
        "        'div': '÷',\n",
        "        'lparen': '(',\n",
        "        'rparen': ')'\n",
        "    }\n",
        "\n",
        "    expr = \"\"\n",
        "    digit_buffer = \"\"\n",
        "    for s in symbols:\n",
        "        if s.isdigit():\n",
        "            digit_buffer += s\n",
        "        else:\n",
        "\n",
        "            if digit_buffer != \"\":\n",
        "                expr += digit_buffer\n",
        "                digit_buffer = \"\"\n",
        "\n",
        "            expr += op_map.get(s, '')\n",
        "    if digit_buffer != \"\":\n",
        "        expr += digit_buffer\n",
        "    return expr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLbDJi6Vbt9u"
      },
      "source": [
        "### Expression Evalutaion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQln60Eabxom"
      },
      "outputs": [],
      "source": [
        "def evaluate_expression(expr_str):\n",
        "\n",
        "    expr_str = expr_str.replace('×','*')\n",
        "    expr_str = expr_str.replace('÷','/')\n",
        "    try:\n",
        "        val = eval(expr_str)\n",
        "    except:\n",
        "        val = 0\n",
        "    return round(val, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcJQE6yos6uf"
      },
      "source": [
        "## OCR based on Clova AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAyobNoIvfOr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9McuG3dt5xdG"
      },
      "outputs": [],
      "source": [
        "class BidirectionalLSTM(nn.Module):\n",
        "    def __init__(self, nIn, nHidden, nOut):\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "        self.rnn = nn.LSTM(nIn, nHidden, num_layers=2, bidirectional=True)\n",
        "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
        "\n",
        "    def forward(self, input):\n",
        "        recurrent, _ = self.rnn(input)\n",
        "        T, b, h = recurrent.size()\n",
        "        t_rec = recurrent.view(T * b, h)\n",
        "        output = self.embedding(t_rec)\n",
        "        output = output.view(T, b, -1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG-Rx4Sk_4tm"
      },
      "outputs": [],
      "source": [
        "class VGG_FeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_channel, output_channel=512):\n",
        "        super(VGG_FeatureExtractor, self).__init__()\n",
        "        self.ConvNet = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # 64\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # 128\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1)),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1)),\n",
        "            nn.Conv2d(512, output_channel, kernel_size=2, stride=1, padding=0),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ConvNet(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCGo4Xgws_JJ"
      },
      "outputs": [],
      "source": [
        "class CRAFTDetector(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(CRAFTDetector, self).__init__()\n",
        "\n",
        "        vgg16_bn = torchvision.models.vgg16_bn(pretrained=pretrained)\n",
        "        self.features = vgg16_bn.features\n",
        "        self.conv1 = nn.Conv2d(512, 128, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(128, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.features(x)\n",
        "        x = F.relu(self.conv1(feat))\n",
        "        score_map = torch.sigmoid(self.conv2(x))  # (N,1,H,W)\n",
        "        return score_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HqBq6uZtCBf"
      },
      "outputs": [],
      "source": [
        "def get_text_box(score_map, threshold=0.5):\n",
        "    score_np = score_map.squeeze().cpu().detach().numpy()\n",
        "    binary = ((score_np > threshold) * 255).astype(np.uint8)\n",
        "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if contours:\n",
        "        box = max(contours, key=lambda cnt: cv2.contourArea(cnt))\n",
        "        x, y, w, h = cv2.boundingRect(box)\n",
        "        return (x, y, w, h)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shaic0S_tE3f"
      },
      "outputs": [],
      "source": [
        "class CRNNRecognizer(nn.Module):\n",
        "    def __init__(self, imgH=32, nc=1, nclass=17, nh=256):\n",
        "        super(CRNNRecognizer, self).__init__()\n",
        "        self.FeatureExtraction = VGG_FeatureExtractor(nc, output_channel=512)\n",
        "        self.SequenceModeling = nn.Sequential(\n",
        "            BidirectionalLSTM(512, nh, nh),\n",
        "            BidirectionalLSTM(nh, nh, nclass)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.FeatureExtraction(x)\n",
        "        b, c, h, w = features.size()\n",
        "        # The expected output height should be 1.\n",
        "        assert h == 1, \"the height of conv features must be 1\"\n",
        "        features = features.squeeze(2)  # shape: (b, c, w)\n",
        "        features = features.permute(2, 0, 1)  # shape: (w, b, c)\n",
        "        output = self.SequenceModeling(features)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3CsDVjptJXE"
      },
      "outputs": [],
      "source": [
        "def ctc_greedy_decoder(output, blank=0):\n",
        "    output = output.cpu().detach().numpy()\n",
        "    pred_indices = np.argmax(output[:, 0, :], axis=1)\n",
        "    print(\"Raw tokens:\", pred_indices)  # Debug print\n",
        "    decoded = []\n",
        "    prev = -1\n",
        "    for idx in pred_indices:\n",
        "        if idx != prev and idx != blank:\n",
        "            decoded.append(idx)\n",
        "        prev = idx\n",
        "    mapping = {\n",
        "        1: '0', 2: '1', 3: '2', 4: '3', 5: '4',\n",
        "        6: '5', 7: '6', 8: '7', 9: '8', 10: '9',\n",
        "        11: '+', 12: '-', 13: '×', 14: '÷', 15: '(', 16: ')'\n",
        "    }\n",
        "    expr = \"\"\n",
        "    for token in decoded:\n",
        "        expr += mapping.get(token, '')\n",
        "    return expr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k95417fhtO-N"
      },
      "outputs": [],
      "source": [
        "def recognize_expression(image_path, craft_model, crnn_model, device):\n",
        "    pil_img = Image.open(image_path).convert(\"RGB\")\n",
        "    craft_transform = T.Compose([\n",
        "        T.Resize((768,768)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
        "    ])\n",
        "    input_tensor = craft_transform(pil_img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        score_map = craft_model(input_tensor)\n",
        "    box = get_text_box(score_map, threshold=0.7)\n",
        "    if box is None:\n",
        "        crop = np.array(pil_img.convert(\"L\"))\n",
        "    else:\n",
        "        x, y, w, h = box\n",
        "        orig_w, orig_h = pil_img.size\n",
        "        scale_x = orig_w / 768.0\n",
        "        scale_y = orig_h / 768.0\n",
        "        x = int(x * scale_x)\n",
        "        y = int(y * scale_y)\n",
        "        w = int(w * scale_x)\n",
        "        h = int(h * scale_y)\n",
        "        img_np = np.array(pil_img.convert(\"L\"))\n",
        "        crop = img_np[y:y+h, x:x+w]\n",
        "    # Save crop for debugging\n",
        "    cv2.imwrite(\"debug_crop.png\", crop)\n",
        "    if crop.size == 0:\n",
        "        return \"\"\n",
        "    h_crop, w_crop = crop.shape\n",
        "    new_h = 32\n",
        "    new_w = max(1, int(w_crop * new_h / h_crop))\n",
        "    crop_resized = cv2.resize(crop, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
        "    crop_pil = Image.fromarray(crop_resized).convert(\"L\")\n",
        "    crnn_transform = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "    crnn_input = crnn_transform(crop_pil).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        crnn_out = crnn_model(crnn_input)\n",
        "    # Adjust blank token index: use blank=16 (instead of 0)\n",
        "    recognized_expr = ctc_greedy_decoder(crnn_out, blank=16)\n",
        "    recognized_expr = recognized_expr.replace(\" \", \"\")\n",
        "    return recognized_expr\n",
        "\n",
        "def ctc_greedy_decoder(output, blank=16):\n",
        "    output = output.cpu().detach().numpy()\n",
        "    pred_indices = np.argmax(output[:,0,:], axis=1)\n",
        "    decoded = []\n",
        "    prev = -1\n",
        "    for idx in pred_indices:\n",
        "        if idx != prev and idx != blank:\n",
        "            decoded.append(idx)\n",
        "        prev = idx\n",
        "    mapping = {\n",
        "        1: '0', 2: '1', 3: '2', 4: '3', 5: '4',\n",
        "        6: '5', 7: '6', 8: '7', 9: '8', 10: '9',\n",
        "        11: '+', 12: '-', 13: '×', 14: '÷', 15: '(', 16: ')'\n",
        "    }\n",
        "    expr = \"\".join(mapping.get(token, '') for token in decoded)\n",
        "    return expr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE03xX5Vb6jy"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRo7qzhgcM7H"
      },
      "source": [
        "The Pipeline of our model is implemented below in the following order:\n",
        "\n",
        "\n",
        "1.   Train typed/handwritten model\n",
        "2.   Train single-char model\n",
        "3.   For test images:\n",
        "     - typed/handwritten classification\n",
        "     - if typed or handwritten, we do character segmentation\n",
        "     - classify each char\n",
        "     - build expression string\n",
        "     - evaluate\n",
        "     - output in submission.csv\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cByLQCtIb8Dc"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    print(\"Reading train CSV...\")\n",
        "    df = read_train_csv(TRAIN_CSV)\n",
        "    print(\"Balancing typed vs handwritten data...\")\n",
        "    df_balanced = balance_dataset(df)\n",
        "\n",
        "    print(\"Creating typed/handwritten dataset...\")\n",
        "    full_dataset = ExpressionTypeDataset(df_balanced, TRAIN_DIR, transform=train_transforms)\n",
        "    n_data = len(full_dataset)\n",
        "    n_val = int(VAL_SPLIT * n_data)\n",
        "    n_train = n_data - n_val\n",
        "    train_ds, val_ds = random_split(full_dataset, [n_train, n_val])\n",
        "    val_ds.dataset.transform = val_transforms\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(\"Creating typed/handwritten model...\")\n",
        "    type_model = create_type_model(num_classes=2).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(type_model.parameters(), lr=LR)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_weights = None\n",
        "\n",
        "    print(\"Training typed/handwritten classifier...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        type_model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for imgs, labels, _ in train_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            out = type_model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            _, preds = torch.max(out, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = correct / total\n",
        "\n",
        "        type_model.eval()\n",
        "        val_loss_ = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels, _ in val_loader:\n",
        "                imgs = imgs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "                out = type_model(imgs)\n",
        "                loss = criterion(out, labels)\n",
        "                val_loss_ += loss.item() * imgs.size(0)\n",
        "                _, preds = torch.max(out, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "        val_loss = val_loss_ / val_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_weights = {k: v.cpu() for k, v in type_model.state_dict().items()}\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "    if best_weights is not None:\n",
        "        type_model.load_state_dict({k: v.to(DEVICE) for k, v in best_weights.items()})\n",
        "    print(f\"Best typed/handwritten validation accuracy: {best_acc*100:.2f}%\")\n",
        "\n",
        "    print(\"Loading OCR models (CRAFT + CRNN)...\")\n",
        "    craft_model = CRAFTDetector(pretrained=True).to(DEVICE)\n",
        "    crnn_model = CRNNRecognizer(imgH=32, nc=1, nclass=17, nh=256).to(DEVICE)\n",
        "\n",
        "    pretrained_path = \"/content/drive/MyDrive/OCR_data/None-VGG-BiLSTM-CTC.pth\"\n",
        "    if os.path.exists(pretrained_path):\n",
        "        crnn_model.load_state_dict(torch.load(pretrained_path, map_location=DEVICE), strict=False)\n",
        "        print(\"Successfully loaded pretrained CRNN weights from Clova AI model.\")\n",
        "    else:\n",
        "        print(\"Pretrained CRNN weights not found at\", pretrained_path)\n",
        "\n",
        "\n",
        "    craft_model.eval()\n",
        "    crnn_model.eval()\n",
        "\n",
        "    print(\"Predicting on test data with OCR...\")\n",
        "    test_files = sorted(os.listdir(TEST_DIR), key=lambda x: int(os.path.splitext(x)[0]))\n",
        "    submission_rows = []\n",
        "    type_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for file in test_files:\n",
        "            test_path = os.path.join(TEST_DIR, file)\n",
        "\n",
        "            pil_img = Image.open(test_path).convert(\"RGB\")\n",
        "            type_input = val_transforms(pil_img).unsqueeze(0).to(DEVICE)\n",
        "            out = type_model(type_input)\n",
        "            _, pred_label = torch.max(out, 1)\n",
        "            pred_type = pred_label.item()\n",
        "\n",
        "\n",
        "            recognized_expr = recognize_expression(test_path, craft_model, crnn_model, DEVICE)\n",
        "            print(f\"For image {file}, recognized expression: '{recognized_expr}'\")\n",
        "\n",
        "            expr_for_eval = recognized_expr.replace(\"×\", \"*\").replace(\"÷\", \"/\")\n",
        "            try:\n",
        "                result_value = eval(expr_for_eval)\n",
        "            except Exception as e:\n",
        "                result_value = 0\n",
        "            answer = round(result_value, 2)\n",
        "\n",
        "            submission_rows.append([str(pred_type), f\"{answer:.2f}\"])\n",
        "\n",
        "    with open(SUBMISSION_CSV, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"type\", \"answer\"])\n",
        "        for row in submission_rows:\n",
        "            writer.writerow(row)\n",
        "\n",
        "    print(f\"Submission saved to {SUBMISSION_CSV}\")\n",
        "    print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_YoqjPScyoP",
        "outputId": "1ea277e8-a5d2-49d5-d083-7bdaf17df19c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading train CSV...\n",
            "Balancing typed vs handwritten data...\n",
            "Creating typed/handwritten dataset...\n",
            "Creating typed/handwritten model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 199MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training typed/handwritten classifier...\n",
            "Epoch [1/20] - Train Loss: 0.0311, Train Acc: 98.83% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [2/20] - Train Loss: 0.0008, Train Acc: 100.00% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [3/20] - Train Loss: 0.0003, Train Acc: 100.00% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [4/20] - Train Loss: 0.0068, Train Acc: 99.61% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [5/20] - Train Loss: 0.0002, Train Acc: 100.00% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [6/20] - Train Loss: 0.0243, Train Acc: 99.14% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [7/20] - Train Loss: 0.0004, Train Acc: 100.00% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [8/20] - Train Loss: 0.0218, Train Acc: 99.22% | Val Loss: 2.6406, Val Acc: 51.41%\n",
            "Epoch [9/20] - Train Loss: 0.0106, Train Acc: 99.61% | Val Loss: 0.0065, Val Acc: 100.00%\n",
            "Epoch [10/20] - Train Loss: 0.0016, Train Acc: 100.00% | Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Epoch [11/20] - Train Loss: 0.0002, Train Acc: 100.00% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [12/20] - Train Loss: 0.0002, Train Acc: 100.00% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [13/20] - Train Loss: 0.0004, Train Acc: 100.00% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [14/20] - Train Loss: 0.0001, Train Acc: 100.00% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [15/20] - Train Loss: 0.0448, Train Acc: 98.28% | Val Loss: 0.0018, Val Acc: 100.00%\n",
            "Epoch [16/20] - Train Loss: 0.0112, Train Acc: 99.30% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [17/20] - Train Loss: 0.0366, Train Acc: 98.20% | Val Loss: 0.0001, Val Acc: 100.00%\n",
            "Epoch [18/20] - Train Loss: 0.0078, Train Acc: 99.45% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [19/20] - Train Loss: 0.0003, Train Acc: 100.00% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Epoch [20/20] - Train Loss: 0.0015, Train Acc: 100.00% | Val Loss: 0.0000, Val Acc: 100.00%\n",
            "Best typed/handwritten validation accuracy: 100.00%\n",
            "Loading OCR models (CRAFT + CRNN)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 69.9MB/s]\n",
            "<ipython-input-24-915704bc603a>:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  crnn_model.load_state_dict(torch.load(pretrained_path, map_location=DEVICE), strict=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded pretrained CRNN weights from Clova AI model.\n",
            "Predicting on test data with OCR...\n",
            "For image 0.png, recognized expression: '7×5×5'\n",
            "For image 1.png, recognized expression: '7×5×5'\n",
            "For image 2.png, recognized expression: '7×5×5'\n",
            "For image 3.png, recognized expression: '7×5×5'\n",
            "For image 4.png, recognized expression: '7×5×5'\n",
            "For image 5.png, recognized expression: '7×5×5'\n",
            "For image 6.png, recognized expression: '7×5×5'\n",
            "For image 7.png, recognized expression: '7×5×5'\n",
            "For image 8.png, recognized expression: '7×5×5'\n",
            "For image 9.png, recognized expression: '7×5×5'\n",
            "For image 10.png, recognized expression: '7×5×5'\n",
            "For image 11.png, recognized expression: '7×5×5'\n",
            "For image 12.png, recognized expression: '7×5×5'\n",
            "For image 13.png, recognized expression: '7×5×5'\n",
            "For image 14.png, recognized expression: '7×5×5'\n",
            "For image 15.png, recognized expression: '7×5×5'\n",
            "For image 16.png, recognized expression: '7×5×5'\n",
            "For image 17.png, recognized expression: '7×5×5'\n",
            "For image 18.png, recognized expression: '7×5×5'\n",
            "For image 19.png, recognized expression: '7×5×5'\n",
            "For image 20.png, recognized expression: '7×5×5'\n",
            "For image 21.png, recognized expression: '7×5×5'\n",
            "For image 22.png, recognized expression: '7×5×5'\n",
            "For image 23.png, recognized expression: '7×5×5'\n",
            "For image 24.png, recognized expression: '7×5×5'\n",
            "For image 25.png, recognized expression: '7×5×5'\n",
            "For image 26.png, recognized expression: '7×5×5'\n",
            "For image 27.png, recognized expression: '7×5×5'\n",
            "For image 28.png, recognized expression: '7×5×5'\n",
            "For image 29.png, recognized expression: '7×5×5'\n",
            "For image 30.png, recognized expression: '7×5×5'\n",
            "For image 31.png, recognized expression: '7×5×5'\n",
            "For image 32.png, recognized expression: '7×5×5'\n",
            "For image 33.png, recognized expression: '7×5×5'\n",
            "For image 34.png, recognized expression: '7×5×5'\n",
            "For image 35.png, recognized expression: '7×5×5'\n",
            "For image 36.png, recognized expression: '7×5×5'\n",
            "For image 37.png, recognized expression: '7×5×5'\n",
            "For image 38.png, recognized expression: '7×5×5'\n",
            "For image 39.png, recognized expression: '7×5×5'\n",
            "For image 40.png, recognized expression: '7×5×5'\n",
            "For image 41.png, recognized expression: '7×5×5'\n",
            "For image 42.png, recognized expression: '7×5×5'\n",
            "For image 43.png, recognized expression: '7×5×5'\n",
            "For image 44.png, recognized expression: '7×5×5'\n",
            "For image 45.png, recognized expression: '7×5×5'\n",
            "For image 46.png, recognized expression: '7×5×5'\n",
            "For image 47.png, recognized expression: '7×5×5'\n",
            "For image 48.png, recognized expression: '7×5×5'\n",
            "For image 49.png, recognized expression: '7×5×5'\n",
            "For image 50.png, recognized expression: '7×5×5'\n",
            "For image 51.png, recognized expression: '7×5×5'\n",
            "For image 52.png, recognized expression: '7×5×5'\n",
            "For image 53.png, recognized expression: '7×5×5'\n",
            "For image 54.png, recognized expression: '7×5×5'\n",
            "For image 55.png, recognized expression: '7×5×5'\n",
            "For image 56.png, recognized expression: '7×5×5'\n",
            "For image 57.png, recognized expression: '7×5×5'\n",
            "For image 58.png, recognized expression: '7×5×5'\n",
            "For image 59.png, recognized expression: '7×5×5'\n",
            "For image 60.png, recognized expression: '7×5×5'\n",
            "For image 61.png, recognized expression: '7×5×5'\n",
            "For image 62.png, recognized expression: '7×5×5'\n",
            "For image 63.png, recognized expression: '7×5×5'\n",
            "For image 64.png, recognized expression: '7×5×5'\n",
            "For image 65.png, recognized expression: '7×5×5'\n",
            "For image 66.png, recognized expression: '7×5×5'\n",
            "For image 67.png, recognized expression: '7×5×5'\n",
            "For image 68.png, recognized expression: '7×5×5'\n",
            "For image 69.png, recognized expression: '7×5×5'\n",
            "For image 70.png, recognized expression: '7×5×5'\n",
            "For image 71.png, recognized expression: '7×5×5'\n",
            "For image 72.png, recognized expression: '7×5×5'\n",
            "For image 73.png, recognized expression: '7×5×5'\n",
            "For image 74.png, recognized expression: '7×5×5'\n",
            "For image 75.png, recognized expression: '7×5×5'\n",
            "For image 76.png, recognized expression: '7×5×5'\n",
            "For image 77.png, recognized expression: '7×5×5'\n",
            "For image 78.png, recognized expression: '7×5×5'\n",
            "For image 79.png, recognized expression: '7×5×5'\n",
            "For image 80.png, recognized expression: '7×5×5'\n",
            "For image 81.png, recognized expression: '7×5×5'\n",
            "For image 82.png, recognized expression: '7×5×5'\n",
            "For image 83.png, recognized expression: '7×5×5'\n",
            "For image 84.png, recognized expression: '7×5×5'\n",
            "For image 85.png, recognized expression: '7×5×5'\n",
            "For image 86.png, recognized expression: '7×5×5'\n",
            "For image 87.png, recognized expression: '7×5×5'\n",
            "For image 88.png, recognized expression: '7×5×5'\n",
            "For image 89.png, recognized expression: '7×5×5'\n",
            "For image 90.png, recognized expression: '7×5×5'\n",
            "For image 91.png, recognized expression: '7×5×5'\n",
            "For image 92.png, recognized expression: '7×5×5'\n",
            "For image 93.png, recognized expression: '7×5×5'\n",
            "For image 94.png, recognized expression: '7×5×5'\n",
            "For image 95.png, recognized expression: '7×5×5'\n",
            "For image 96.png, recognized expression: '7×5×5'\n",
            "For image 97.png, recognized expression: '7×5×5'\n",
            "For image 98.png, recognized expression: '7×5×5'\n",
            "For image 99.png, recognized expression: '7×5×5'\n",
            "For image 100.png, recognized expression: '7×5×5'\n",
            "For image 101.png, recognized expression: '7×5×5'\n",
            "For image 102.png, recognized expression: '7×5×5'\n",
            "For image 103.png, recognized expression: '7×5×5'\n",
            "For image 104.png, recognized expression: '7×5×5'\n",
            "For image 105.png, recognized expression: '7×5×5'\n",
            "For image 106.png, recognized expression: '7×5×5'\n",
            "For image 107.png, recognized expression: '7×5×5'\n",
            "For image 108.png, recognized expression: '7×5×5'\n",
            "For image 109.png, recognized expression: '7×5×5'\n",
            "For image 110.png, recognized expression: '7×5×5'\n",
            "For image 111.png, recognized expression: '7×5×5'\n",
            "For image 112.png, recognized expression: '7×5×5'\n",
            "For image 113.png, recognized expression: '7×5×5'\n",
            "For image 114.png, recognized expression: '7×5×5'\n",
            "For image 115.png, recognized expression: '7×5×5'\n",
            "For image 116.png, recognized expression: '7×5×5'\n",
            "For image 117.png, recognized expression: '7×5×5'\n",
            "For image 118.png, recognized expression: '7×5×5'\n",
            "For image 119.png, recognized expression: '7×5×5'\n",
            "For image 120.png, recognized expression: '7×5×5'\n",
            "For image 121.png, recognized expression: '7×5×5'\n",
            "For image 122.png, recognized expression: '7×5×5'\n",
            "For image 123.png, recognized expression: '7×5×5'\n",
            "For image 124.png, recognized expression: '7×5×5'\n",
            "For image 125.png, recognized expression: '7×5×5'\n",
            "For image 126.png, recognized expression: '7×5×5'\n",
            "For image 127.png, recognized expression: '7×5×5'\n",
            "For image 128.png, recognized expression: '7×5×5'\n",
            "For image 129.png, recognized expression: '7×5×5'\n",
            "For image 130.png, recognized expression: '7×5×5'\n",
            "For image 131.png, recognized expression: '7×5×5'\n",
            "For image 132.png, recognized expression: '7×5×5'\n",
            "For image 133.png, recognized expression: '7×5×5'\n",
            "For image 134.png, recognized expression: '7×5×5'\n",
            "For image 135.png, recognized expression: '7×5×5'\n",
            "For image 136.png, recognized expression: '7×5×5'\n",
            "For image 137.png, recognized expression: '7×5×5'\n",
            "For image 138.png, recognized expression: '7×5×5'\n",
            "For image 139.png, recognized expression: '7×5×5'\n",
            "For image 140.png, recognized expression: '7×5×5'\n",
            "For image 141.png, recognized expression: '7×5×5'\n",
            "For image 142.png, recognized expression: '7×5×5'\n",
            "For image 143.png, recognized expression: '7×5×5'\n",
            "For image 144.png, recognized expression: '7×5×5'\n",
            "For image 145.png, recognized expression: '7×5×5'\n",
            "For image 146.png, recognized expression: '7×5×5'\n",
            "For image 147.png, recognized expression: '7×5×5'\n",
            "For image 148.png, recognized expression: '7×5×5'\n",
            "For image 149.png, recognized expression: '7×5×5'\n",
            "For image 150.png, recognized expression: '7×5×5'\n",
            "For image 151.png, recognized expression: '7×5×5'\n",
            "For image 152.png, recognized expression: '7×5×5'\n",
            "For image 153.png, recognized expression: '7×5×5'\n",
            "For image 154.png, recognized expression: '7×5×5'\n",
            "For image 155.png, recognized expression: '7×5×5'\n",
            "For image 156.png, recognized expression: '7×5×5'\n",
            "For image 157.png, recognized expression: '7×5×5'\n",
            "For image 158.png, recognized expression: '7×5×5'\n",
            "For image 159.png, recognized expression: '7×5×5'\n",
            "For image 160.png, recognized expression: '7×5×5'\n",
            "For image 161.png, recognized expression: '7×5×5'\n",
            "For image 162.png, recognized expression: '7×5×5'\n",
            "For image 163.png, recognized expression: '7×5×5'\n",
            "For image 164.png, recognized expression: '7×5×5'\n",
            "For image 165.png, recognized expression: '7×5×5'\n",
            "For image 166.png, recognized expression: '7×5×5'\n",
            "For image 167.png, recognized expression: '7×5×5'\n",
            "For image 168.png, recognized expression: '7×5×5'\n",
            "For image 169.png, recognized expression: '7×5×5'\n",
            "For image 170.png, recognized expression: '7×5×5'\n",
            "For image 171.png, recognized expression: '7×5×5'\n",
            "For image 172.png, recognized expression: '7×5×5'\n",
            "For image 173.png, recognized expression: '7×5×5'\n",
            "For image 174.png, recognized expression: '7×5×5'\n",
            "For image 175.png, recognized expression: '7×5×5'\n",
            "For image 176.png, recognized expression: '7×5×5'\n",
            "For image 177.png, recognized expression: '7×5×5'\n",
            "For image 178.png, recognized expression: '7×5×5'\n",
            "For image 179.png, recognized expression: '7×5×5'\n",
            "For image 180.png, recognized expression: '7×5×5'\n",
            "For image 181.png, recognized expression: '7×5×5'\n",
            "For image 182.png, recognized expression: '7×5×5'\n",
            "For image 183.png, recognized expression: '7×5×5'\n",
            "For image 184.png, recognized expression: '7×5×5'\n",
            "For image 185.png, recognized expression: '7×5'\n",
            "For image 186.png, recognized expression: '7×5×5'\n",
            "For image 187.png, recognized expression: '7×5×5'\n",
            "For image 188.png, recognized expression: '7×5×5'\n",
            "For image 189.png, recognized expression: '7×5×5'\n",
            "For image 190.png, recognized expression: '7×5×5'\n",
            "For image 191.png, recognized expression: '7×5×5'\n",
            "For image 192.png, recognized expression: '7×5×5'\n",
            "For image 193.png, recognized expression: '7×5×5'\n",
            "For image 194.png, recognized expression: '7×5×5'\n",
            "For image 195.png, recognized expression: '7×5×5'\n",
            "For image 196.png, recognized expression: '7×5×5'\n",
            "For image 197.png, recognized expression: '7×5×5'\n",
            "For image 198.png, recognized expression: '7×5×5'\n",
            "For image 199.png, recognized expression: '7×5×5'\n",
            "For image 200.png, recognized expression: '7×5×5'\n",
            "For image 201.png, recognized expression: '7×5×5'\n",
            "For image 202.png, recognized expression: '7×5×5'\n",
            "For image 203.png, recognized expression: '7×5×5'\n",
            "For image 204.png, recognized expression: '7×5'\n",
            "For image 205.png, recognized expression: '7×5×5'\n",
            "For image 206.png, recognized expression: '7×5×5'\n",
            "For image 207.png, recognized expression: '7×5×5'\n",
            "For image 208.png, recognized expression: '7×5×5'\n",
            "For image 209.png, recognized expression: '7×5×5'\n",
            "For image 210.png, recognized expression: '7×5×5'\n",
            "For image 211.png, recognized expression: '7×5×5'\n",
            "For image 212.png, recognized expression: '7×5×5'\n",
            "For image 213.png, recognized expression: '7×5×5'\n",
            "For image 214.png, recognized expression: '7×5×5'\n",
            "For image 215.png, recognized expression: '7×5×5'\n",
            "For image 216.png, recognized expression: '7×5×5'\n",
            "For image 217.png, recognized expression: '7×5×5'\n",
            "For image 218.png, recognized expression: '7×5×5'\n",
            "For image 219.png, recognized expression: '7×5×5'\n",
            "For image 220.png, recognized expression: '7×5×5'\n",
            "For image 221.png, recognized expression: '7×5×5'\n",
            "For image 222.png, recognized expression: '7×5×5'\n",
            "For image 223.png, recognized expression: '7×5×5'\n",
            "For image 224.png, recognized expression: '7×5×5'\n",
            "For image 225.png, recognized expression: '7×5×5'\n",
            "For image 226.png, recognized expression: '7×5×5'\n",
            "For image 227.png, recognized expression: '7×5×5'\n",
            "For image 228.png, recognized expression: '7×5×5'\n",
            "For image 229.png, recognized expression: '7×5×5'\n",
            "For image 230.png, recognized expression: '7×5×5'\n",
            "For image 231.png, recognized expression: '7×5×5'\n",
            "For image 232.png, recognized expression: '7×5×5'\n",
            "For image 233.png, recognized expression: '7×5×5'\n",
            "For image 234.png, recognized expression: '7×5×5'\n",
            "For image 235.png, recognized expression: '7×5×5'\n",
            "For image 236.png, recognized expression: '7×5×5'\n",
            "For image 237.png, recognized expression: '7×5×5'\n",
            "For image 238.png, recognized expression: '7×5×5'\n",
            "For image 239.png, recognized expression: '7×5×5'\n",
            "For image 240.png, recognized expression: '7×5×5'\n",
            "For image 241.png, recognized expression: '7×5×5'\n",
            "For image 242.png, recognized expression: '7×5×5'\n",
            "For image 243.png, recognized expression: '7×5×5'\n",
            "For image 244.png, recognized expression: '7×5×5'\n",
            "For image 245.png, recognized expression: '7×5×5'\n",
            "For image 246.png, recognized expression: '7×5×5'\n",
            "For image 247.png, recognized expression: '7×5×5'\n",
            "For image 248.png, recognized expression: '7×5×5'\n",
            "For image 249.png, recognized expression: '7×5×5'\n",
            "For image 250.png, recognized expression: '7×5×5'\n",
            "For image 251.png, recognized expression: '7×5×5'\n",
            "For image 252.png, recognized expression: '7×5×5'\n",
            "For image 253.png, recognized expression: '7×5×5'\n",
            "For image 254.png, recognized expression: '7×5×5'\n",
            "For image 255.png, recognized expression: '7×5×5'\n",
            "For image 256.png, recognized expression: '7×5'\n",
            "For image 257.png, recognized expression: '7×5×5'\n",
            "For image 258.png, recognized expression: '7×5×5'\n",
            "For image 259.png, recognized expression: '7×5×5'\n",
            "For image 260.png, recognized expression: '7×5'\n",
            "For image 261.png, recognized expression: '7×5×5'\n",
            "For image 262.png, recognized expression: '7×5×5'\n",
            "For image 263.png, recognized expression: '7×5×5'\n",
            "For image 264.png, recognized expression: '7×5×5'\n",
            "For image 265.png, recognized expression: '7×5×5'\n",
            "For image 266.png, recognized expression: '7×5×5'\n",
            "For image 267.png, recognized expression: '7×5×5'\n",
            "For image 268.png, recognized expression: '7×5×5'\n",
            "For image 269.png, recognized expression: '7×5×5'\n",
            "For image 270.png, recognized expression: '7×5'\n",
            "For image 271.png, recognized expression: '7×5×5'\n",
            "For image 272.png, recognized expression: '7×5×5'\n",
            "For image 273.png, recognized expression: '7×5×5'\n",
            "For image 274.png, recognized expression: '7×5×5'\n",
            "For image 275.png, recognized expression: '7×5×5'\n",
            "For image 276.png, recognized expression: '7×5×5'\n",
            "For image 277.png, recognized expression: '7×5×5'\n",
            "For image 278.png, recognized expression: '7×5×5'\n",
            "For image 279.png, recognized expression: '7×5×5'\n",
            "For image 280.png, recognized expression: '7×5×5'\n",
            "For image 281.png, recognized expression: '7×5×5'\n",
            "For image 282.png, recognized expression: '7×5×5'\n",
            "For image 283.png, recognized expression: '7×5×5'\n",
            "For image 284.png, recognized expression: '7×5×5'\n",
            "For image 285.png, recognized expression: '7×5×5'\n",
            "For image 286.png, recognized expression: '7×5×5'\n",
            "For image 287.png, recognized expression: '7×5×5'\n",
            "For image 288.png, recognized expression: '7×5×5'\n",
            "For image 289.png, recognized expression: '7×5×5'\n",
            "For image 290.png, recognized expression: '7×5×5'\n",
            "For image 291.png, recognized expression: '7×5×5'\n",
            "For image 292.png, recognized expression: '7×5×5'\n",
            "For image 293.png, recognized expression: '7×5×5'\n",
            "For image 294.png, recognized expression: '7×5×5'\n",
            "For image 295.png, recognized expression: '7×5×5'\n",
            "For image 296.png, recognized expression: '7×5×5'\n",
            "For image 297.png, recognized expression: '7×5×5'\n",
            "For image 298.png, recognized expression: '7×5×5'\n",
            "For image 299.png, recognized expression: '7×5×5'\n",
            "For image 300.png, recognized expression: '7×5×5'\n",
            "For image 301.png, recognized expression: '7×5×5'\n",
            "For image 302.png, recognized expression: '7×5×5'\n",
            "For image 303.png, recognized expression: '7×5×5'\n",
            "For image 304.png, recognized expression: '7×5×5'\n",
            "For image 305.png, recognized expression: '7×5×5'\n",
            "For image 306.png, recognized expression: '7×5×5'\n",
            "For image 307.png, recognized expression: '7×5×5'\n",
            "For image 308.png, recognized expression: '7×5×5'\n",
            "For image 309.png, recognized expression: '7×5×5'\n",
            "For image 310.png, recognized expression: '7×5×5'\n",
            "For image 311.png, recognized expression: '7×5×5'\n",
            "Submission saved to /content/drive/MyDrive/OCR_data/submission.csv\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZBeanDVEXQ56",
        "DVB2rc0dX6dm",
        "YS0_E0P8X-5d",
        "srLOqST9YFWV",
        "ks1aNxYAZdcL",
        "nlxetw6FZjOn"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}